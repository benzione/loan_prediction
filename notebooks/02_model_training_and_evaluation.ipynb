{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 2. Model Training and Evaluation\n",
        "\n",
        "This notebook implements comprehensive preprocessing, feature engineering, and model training based on insights from the EDA analysis in notebook 01.\n",
        "\n",
        "## Key EDA Findings to Address:\n",
        "- **Class Imbalance**: 16.1% enrollment rate\n",
        "- **Multicollinearity**: 5 highly correlated feature pairs  \n",
        "- **Missing Values**: 9 features with missing data\n",
        "- **Top Predictive Features**: total_principal_ratio, total_accounts_in_collections, principal_at_placement\n",
        "- **Outliers**: 8 features have >5% outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Configuration loaded from: /Users/eyalbenzion/Library/CloudStorage/GoogleDrive-eyal.benzion@gmail.com/My Drive/work_code/bounce_loan_prediction/config.ini\n",
            "🚀 Model Training and Evaluation Setup Complete!\n",
            "📊 Target column: enrolled_to_plan_in_180\n",
            "🎯 Test size: 0.2\n",
            "🎲 Random state: 42\n",
            "✅ Custom sampling functions loaded (replacing imblearn)\n"
          ]
        }
      ],
      "source": [
        "# Import libraries and setup\n",
        "import sys\n",
        "import os\n",
        "project_root = os.path.abspath('..')\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
        "                           roc_curve, precision_recall_curve, average_precision_score,\n",
        "                           f1_score, precision_score, recall_score, accuracy_score)\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import project modules\n",
        "from src.data.snowflake_connector import get_snowflake_engine, fetch_data\n",
        "from src.config import get_config\n",
        "from src.features.build_features import preprocess_data, create_features\n",
        "\n",
        "# Load configuration\n",
        "config = get_config()\n",
        "target_column = config['model']['target_column'].lower()\n",
        "\n",
        "print(\"🚀 Model Training and Evaluation Setup Complete!\")\n",
        "print(f\"📊 Target column: {target_column}\")\n",
        "print(f\"🎯 Test size: {config['model']['test_size']}\")\n",
        "print(f\"🎲 Random state: {config['model']['random_state']}\")\n",
        "\n",
        "# Set plotting style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Simple sampling functions to replace imblearn\n",
        "def random_oversample(X, y, random_state=42):\n",
        "    \"\"\"Simple random oversampling implementation\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Count classes\n",
        "    unique_classes, counts = np.unique(y, return_counts=True)\n",
        "    max_count = counts.max()\n",
        "    \n",
        "    # Oversample minority classes\n",
        "    X_resampled = []\n",
        "    y_resampled = []\n",
        "    \n",
        "    for class_label in unique_classes:\n",
        "        class_indices = np.where(y == class_label)[0]\n",
        "        class_count = len(class_indices)\n",
        "        \n",
        "        if class_count < max_count:\n",
        "            # Oversample\n",
        "            needed_samples = max_count - class_count\n",
        "            oversample_indices = np.random.choice(class_indices, needed_samples, replace=True)\n",
        "            all_indices = np.concatenate([class_indices, oversample_indices])\n",
        "        else:\n",
        "            all_indices = class_indices\n",
        "            \n",
        "        X_resampled.append(X.iloc[all_indices])\n",
        "        y_resampled.append(y.iloc[all_indices])\n",
        "    \n",
        "    X_resampled = pd.concat(X_resampled, ignore_index=True)\n",
        "    y_resampled = pd.concat(y_resampled, ignore_index=True)\n",
        "    \n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "def random_undersample(X, y, random_state=42):\n",
        "    \"\"\"Simple random undersampling implementation\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Count classes\n",
        "    unique_classes, counts = np.unique(y, return_counts=True)\n",
        "    min_count = counts.min()\n",
        "    \n",
        "    # Undersample majority classes\n",
        "    X_resampled = []\n",
        "    y_resampled = []\n",
        "    \n",
        "    for class_label in unique_classes:\n",
        "        class_indices = np.where(y == class_label)[0]\n",
        "        \n",
        "        if len(class_indices) > min_count:\n",
        "            # Undersample\n",
        "            selected_indices = np.random.choice(class_indices, min_count, replace=False)\n",
        "        else:\n",
        "            selected_indices = class_indices\n",
        "            \n",
        "        X_resampled.append(X.iloc[selected_indices])\n",
        "        y_resampled.append(y.iloc[selected_indices])\n",
        "    \n",
        "    X_resampled = pd.concat(X_resampled, ignore_index=True)\n",
        "    y_resampled = pd.concat(y_resampled, ignore_index=True)\n",
        "    \n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "print(\"✅ Custom sampling functions loaded (replacing imblearn)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Data Loading and Initial Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📥 Loading data from Snowflake...\n",
            "2025-07-03 09:13:36,559 - src.data.snowflake_connector - INFO - Connecting to Snowflake account: fc26424.us-east-2.aws\n",
            "2025-07-03 09:13:36,562 - src.data.snowflake_connector - INFO - Using database: BOUNCE_ASSIGNMENT, schema: PUBLIC, warehouse: BOUNCE_GUEST\n",
            "2025-07-03 09:13:37,876 - src.data.snowflake_connector - INFO - Successfully created Snowflake engine.\n",
            "2025-07-03 09:13:51,207 - src.data.snowflake_connector - INFO - Successfully fetched data from PREDICT_RECOVERY.\n",
            "✅ Data loaded: (34628, 21)\n",
            "📊 Original dataset size: 34,628 rows, 21 columns\n",
            "\n",
            "🔧 Applying Comprehensive Preprocessing Pipeline...\n",
            "🎯 Using EDA-based preprocessing from src.features.build_features\n",
            "2025-07-03 09:13:51,208 - src.features.build_features - INFO - Starting comprehensive data preprocessing pipeline...\n",
            "2025-07-03 09:13:51,209 - src.features.build_features - INFO - Cleaning target variable: enrolled_to_plan_in_180\n",
            "2025-07-03 09:13:51,230 - src.features.build_features - INFO - Dropped 2214 rows with missing target values\n",
            "2025-07-03 09:13:51,233 - src.features.build_features - INFO - Converted target variable from string to boolean\n",
            "2025-07-03 09:13:51,235 - src.features.build_features - INFO - Target distribution: {False: np.int64(27191), True: np.int64(5223)}\n",
            "2025-07-03 09:13:51,236 - src.features.build_features - INFO - Enrollment rate: 0.161\n",
            "2025-07-03 09:13:51,239 - src.features.build_features - INFO - Removed identifier columns: ['account_public_id']\n",
            "2025-07-03 09:13:51,240 - src.features.build_features - INFO - Handling multicollinearity...\n",
            "2025-07-03 09:13:51,243 - src.features.build_features - INFO - Removed highly correlated features: ['days_from_dq_to_placement', 'days_from_last_payment_to_assignment', 'total_at_placement']\n",
            "2025-07-03 09:13:51,244 - src.features.build_features - INFO - Handling missing values...\n",
            "2025-07-03 09:13:51,255 - src.features.build_features - INFO - Found missing values in 8 features\n",
            "2025-07-03 09:13:51,258 - src.features.build_features - INFO - Filled sum_of_balance_amount_installment_loans (99 values, 0.31%) with median: 19283.00\n",
            "2025-07-03 09:13:51,262 - src.features.build_features - INFO - Filled bank_card_credit_utilization_pct (99 values, 0.31%) with median: 77.00\n",
            "2025-07-03 09:13:51,265 - src.features.build_features - INFO - Filled installment_loans_accounts_opened_3_months (99 values, 0.31%) with median: 0.00\n",
            "2025-07-03 09:13:51,268 - src.features.build_features - INFO - Filled revolving_accounts_opened_3_months (99 values, 0.31%) with median: 0.00\n",
            "2025-07-03 09:13:51,270 - src.features.build_features - INFO - Filled thirty_dpd_in_last_24_months (99 values, 0.31%) with median: 0.00\n",
            "2025-07-03 09:13:51,272 - src.features.build_features - INFO - Filled total_accounts_in_collections (99 values, 0.31%) with median: 0.00\n",
            "2025-07-03 09:13:51,274 - src.features.build_features - INFO - Filled total_finance_accounts_balance_gt_0 (99 values, 0.31%) with median: 0.00\n",
            "2025-07-03 09:13:51,276 - src.features.build_features - INFO - Filled total_paid_accounts_6_months (99 values, 0.31%) with median: 0.00\n",
            "2025-07-03 09:13:51,277 - src.features.build_features - INFO - Handling outliers...\n",
            "2025-07-03 09:13:51,282 - src.features.build_features - INFO - Capped 2958 outliers in thirty_dpd_in_last_24_months\n",
            "2025-07-03 09:13:51,286 - src.features.build_features - INFO - Capped 2414 outliers in revolving_accounts_opened_3_months\n",
            "2025-07-03 09:13:51,290 - src.features.build_features - INFO - Capped 2197 outliers in total_finance_accounts_balance_gt_0\n",
            "2025-07-03 09:13:51,297 - src.features.build_features - INFO - Capped 2169 outliers in installment_loans_accounts_opened_3_months\n",
            "2025-07-03 09:13:51,303 - src.features.build_features - INFO - Capped 2053 outliers in days_from_origination_to_co\n",
            "2025-07-03 09:13:51,307 - src.features.build_features - INFO - Capped 2075 outliers in sum_of_balance_amount_installment_loans\n",
            "2025-07-03 09:13:51,308 - src.features.build_features - INFO - Encoding categorical features...\n",
            "2025-07-03 09:13:51,324 - src.features.build_features - INFO - Encoded is_fpd: 2 classes\n",
            "2025-07-03 09:13:51,330 - src.features.build_features - INFO - Encoded group_: 2 classes\n",
            "2025-07-03 09:13:51,333 - src.features.build_features - INFO - Training preprocessing complete. Features: 16, Samples: 32414\n",
            "2025-07-03 09:13:51,334 - src.features.build_features - INFO - Target distribution: {False: 27191, True: 5223}\n",
            "\n",
            "✅ Preprocessing pipeline complete!\n",
            "📊 Final dataset: 32,414 samples, 16 features\n",
            "📈 Target distribution: {False: np.int64(27191), True: np.int64(5223)}\n",
            "🎯 Class balance: 0.161 positive rate\n",
            "\n",
            "📋 PREPROCESSING SUMMARY:\n",
            "  ✅ Removed features: ['days_from_dq_to_placement', 'days_from_last_payment_to_assignment', 'total_at_placement']\n",
            "  ✅ Imputed features: 8\n",
            "  ✅ Outlier capped features: 6\n",
            "  ✅ Encoded categorical features: 2\n",
            "  ✅ Final feature set: ['principal_at_placement', 'total_principal_ratio', 'days_from_co_to_placement', 'days_from_origination_to_placement', 'days_from_origination_to_co', 'is_fpd', 'group_', 'sum_of_balance_amount_installment_loans', 'bank_card_credit_utilization_pct', 'installment_loans_accounts_opened_3_months', 'revolving_accounts_opened_3_months', 'thirty_dpd_in_last_24_months', 'total_accounts_in_collections', 'total_finance_accounts_balance_gt_0', 'total_paid_accounts_6_months', 'age']\n"
          ]
        }
      ],
      "source": [
        "# Load data from Snowflake\n",
        "print(\"📥 Loading data from Snowflake...\")\n",
        "engine = get_snowflake_engine()\n",
        "df_raw = fetch_data(engine, config['snowflake']['table_name'])\n",
        "\n",
        "print(f\"✅ Data loaded: {df_raw.shape}\")\n",
        "print(f\"📊 Original dataset size: {df_raw.shape[0]:,} rows, {df_raw.shape[1]} columns\")\n",
        "\n",
        "# Apply comprehensive preprocessing pipeline from build_features.py\n",
        "print(\"\\n🔧 Applying Comprehensive Preprocessing Pipeline...\")\n",
        "print(\"🎯 Using EDA-based preprocessing from src.features.build_features\")\n",
        "\n",
        "# Apply preprocessing pipeline (includes all EDA-based transformations)\n",
        "X, y, preprocessing_artifacts = preprocess_data(\n",
        "    df_raw, \n",
        "    target_column, \n",
        "    is_training=True\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Preprocessing pipeline complete!\")\n",
        "print(f\"📊 Final dataset: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
        "print(f\"📈 Target distribution: {dict(y.value_counts())}\")\n",
        "print(f\"🎯 Class balance: {y.mean():.3f} positive rate\")\n",
        "\n",
        "# Display preprocessing summary\n",
        "print(f\"\\n📋 PREPROCESSING SUMMARY:\")\n",
        "print(f\"  ✅ Removed features: {preprocessing_artifacts['removed_features']}\")\n",
        "print(f\"  ✅ Imputed features: {len(preprocessing_artifacts['imputation_info'])}\")\n",
        "print(f\"  ✅ Outlier capped features: {len(preprocessing_artifacts['outlier_info'])}\")\n",
        "print(f\"  ✅ Encoded categorical features: {len(preprocessing_artifacts['label_encoders'])}\")\n",
        "print(f\"  ✅ Final feature set: {preprocessing_artifacts['feature_names']}\")\n",
        "\n",
        "# Store preprocessing artifacts for model persistence\n",
        "label_encoders = preprocessing_artifacts['label_encoders']\n",
        "feature_names = preprocessing_artifacts['feature_names']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Optional Feature Engineering\n",
        "\n",
        "The comprehensive preprocessing pipeline has been applied. We can optionally create additional features based on EDA insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎨 Optional Feature Engineering...\n",
            "Creating additional features based on EDA insights...\n",
            "2025-07-03 09:13:51,352 - src.features.build_features - INFO - Creating new features based on EDA insights...\n",
            "2025-07-03 09:13:51,359 - src.features.build_features - INFO - Created loan_size_category feature\n",
            "2025-07-03 09:13:51,363 - src.features.build_features - INFO - Created age_category feature\n",
            "2025-07-03 09:13:51,365 - src.features.build_features - INFO - Created has_collections feature\n",
            "2025-07-03 09:13:51,369 - src.features.build_features - INFO - Created placement_urgency feature\n",
            "2025-07-03 09:13:51,369 - src.features.build_features - INFO - Feature creation complete. New shape: (32414, 20)\n",
            "✅ Created 4 additional features\n",
            "🏷️ Encoding 3 new categorical features...\n",
            "✅ Encoded new feature loan_size_category: 4 classes\n",
            "✅ Encoded new feature age_category: 4 classes\n",
            "✅ Encoded new feature placement_urgency: 3 classes\n",
            "📊 Enhanced feature set: 20 features\n",
            "📋 New features: ['loan_size_category', 'age_category', 'has_collections', 'placement_urgency']\n",
            "\n",
            "✅ Final Feature Set Ready!\n",
            "📊 Shape: (32414, 20)\n",
            "🎯 Target balance: 0.161 positive rate\n"
          ]
        }
      ],
      "source": [
        "# Optional: Create additional features based on EDA insights\n",
        "print(\"🎨 Optional Feature Engineering...\")\n",
        "\n",
        "# Create a DataFrame from preprocessed features for easier manipulation\n",
        "df_features = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# Apply feature creation (this will add new categorical features)\n",
        "print(\"Creating additional features based on EDA insights...\")\n",
        "try:\n",
        "    df_enhanced = create_features(df_features)\n",
        "    \n",
        "    # Check if new features were created\n",
        "    if df_enhanced.shape[1] > df_features.shape[1]:\n",
        "        print(f\"✅ Created {df_enhanced.shape[1] - df_features.shape[1]} additional features\")\n",
        "        \n",
        "        # Handle any new categorical features created\n",
        "        new_features = [col for col in df_enhanced.columns if col not in df_features.columns]\n",
        "        categorical_new = df_enhanced[new_features].select_dtypes(include=['object', 'category']).columns\n",
        "        \n",
        "        if len(categorical_new) > 0:\n",
        "            print(f\"🏷️ Encoding {len(categorical_new)} new categorical features...\")\n",
        "            for feature in categorical_new:\n",
        "                le = LabelEncoder()\n",
        "                df_enhanced[feature] = le.fit_transform(df_enhanced[feature].astype(str))\n",
        "                label_encoders[feature] = le\n",
        "                print(f\"✅ Encoded new feature {feature}: {len(le.classes_)} classes\")\n",
        "        \n",
        "        # Update X with enhanced features\n",
        "        X = df_enhanced.values\n",
        "        feature_names = df_enhanced.columns.tolist()\n",
        "        \n",
        "        print(f\"📊 Enhanced feature set: {len(feature_names)} features\")\n",
        "        print(f\"📋 New features: {new_features}\")\n",
        "    else:\n",
        "        print(\"ℹ️ No additional features created - using preprocessed features\")\n",
        "        X = df_features.values\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Feature creation failed: {e}\")\n",
        "    print(\"📊 Using preprocessed features only\")\n",
        "    X = df_features.values\n",
        "\n",
        "print(f\"\\n✅ Final Feature Set Ready!\")\n",
        "print(f\"📊 Shape: {X.shape}\")\n",
        "print(f\"🎯 Target balance: {y.mean():.3f} positive rate\")\n",
        "\n",
        "# Update feature names for later use\n",
        "final_feature_names = feature_names if 'feature_names' in locals() else preprocessing_artifacts['feature_names']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Train/Test Split and Class Imbalance Handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔀 Splitting preprocessed data into train/test sets...\n",
            "✅ Train set: 25,931 samples, 20 features\n",
            "✅ Test set: 6,483 samples, 20 features\n",
            "📊 Train class balance: 0.161\n",
            "📊 Test class balance: 0.161\n",
            "\n",
            "⚖️ Addressing Class Imbalance...\n",
            "🔄 Applying Random Oversampling...\n",
            "   RandomOver balance: 0.500\n",
            "🔄 Applying Random Undersampling...\n",
            "   RandomUnder balance: 0.500\n",
            "\n",
            "✅ Created 3 sampling strategies for comparison\n"
          ]
        }
      ],
      "source": [
        "# Train/Test Split\n",
        "print(\"🔀 Splitting preprocessed data into train/test sets...\")\n",
        "\n",
        "test_size = float(config['model']['test_size'])\n",
        "random_state = int(config['model']['random_state'])\n",
        "\n",
        "# Convert X to DataFrame if it's numpy array for easier handling\n",
        "if isinstance(X, np.ndarray):\n",
        "    X_df = pd.DataFrame(X, columns=final_feature_names)\n",
        "else:\n",
        "    X_df = X.copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_df, y, \n",
        "    test_size=test_size, \n",
        "    random_state=random_state, \n",
        "    stratify=y  # Maintain class balance in split\n",
        ")\n",
        "\n",
        "print(f\"✅ Train set: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
        "print(f\"✅ Test set: {X_test.shape[0]:,} samples, {X_test.shape[1]} features\")\n",
        "print(f\"📊 Train class balance: {y_train.mean():.3f}\")\n",
        "print(f\"📊 Test class balance: {y_test.mean():.3f}\")\n",
        "\n",
        "# Address Class Imbalance (16.1% positive rate from EDA)\n",
        "print(f\"\\n⚖️ Addressing Class Imbalance...\")\n",
        "\n",
        "# Try different sampling strategies\n",
        "sampling_strategies = {}\n",
        "\n",
        "# 1. Original (no sampling)\n",
        "X_orig, y_orig = X_train.copy(), y_train.copy()\n",
        "sampling_strategies['Original'] = (X_orig, y_orig)\n",
        "\n",
        "# 2. Random Oversampling (custom implementation)\n",
        "print(\"🔄 Applying Random Oversampling...\")\n",
        "X_ros, y_ros = random_oversample(X_train, y_train, random_state=random_state)\n",
        "sampling_strategies['RandomOver'] = (X_ros, y_ros)\n",
        "print(f\"   RandomOver balance: {y_ros.mean():.3f}\")\n",
        "\n",
        "# 3. Random Undersampling (custom implementation)\n",
        "print(\"🔄 Applying Random Undersampling...\")\n",
        "X_rus, y_rus = random_undersample(X_train, y_train, random_state=random_state)\n",
        "sampling_strategies['RandomUnder'] = (X_rus, y_rus)\n",
        "print(f\"   RandomUnder balance: {y_rus.mean():.3f}\")\n",
        "\n",
        "# # 4. Class Weight Balanced (no resampling, just weighted models)\n",
        "# X_weighted, y_weighted = X_train.copy(), y_train.copy()\n",
        "# sampling_strategies['ClassWeighted'] = (X_weighted, y_weighted)\n",
        "# print(f\"   ClassWeighted balance: {y_weighted.mean():.3f} (uses model class_weight)\")\n",
        "\n",
        "print(f\"\\n✅ Created {len(sampling_strategies)} sampling strategies for comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Model Training and Evaluation\n",
        "\n",
        "We'll train multiple algorithms with different sampling strategies and compare their performance using metrics appropriate for imbalanced classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Setting up models for comparison...\n",
            "✅ 4 models ready for training\n",
            "\n",
            "🏋️ Training models with different sampling strategies...\n",
            "\n",
            "📊 Training with Original sampling...\n",
            "  🤖 Training Random Forest...\n",
            "    ✅ F1: 0.086, ROC-AUC: 0.727\n",
            "  🤖 Training Gradient Boosting...\n",
            "    ✅ F1: 0.022, ROC-AUC: 0.739\n",
            "  🤖 Training Logistic Regression...\n",
            "    ✅ F1: 0.382, ROC-AUC: 0.710\n",
            "  🤖 Training Extra Trees...\n"
          ]
        }
      ],
      "source": [
        "# Define models to compare\n",
        "print(\"🤖 Setting up models for comparison...\")\n",
        "\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=100, \n",
        "        random_state=random_state, \n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(\n",
        "        n_estimators=100, \n",
        "        random_state=random_state\n",
        "    ),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        random_state=random_state, \n",
        "        class_weight='balanced',\n",
        "        max_iter=1000\n",
        "    ),\n",
        "    'Extra Trees': ExtraTreesClassifier(\n",
        "        n_estimators=100, \n",
        "        random_state=random_state, \n",
        "        class_weight='balanced'\n",
        "    )\n",
        "}\n",
        "\n",
        "print(f\"✅ {len(models)} models ready for training\")\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model_performance(model, X_test, y_test, model_name, sampling_strategy):\n",
        "    \"\"\"Comprehensive model evaluation for imbalanced classification\"\"\"\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Sampling': sampling_strategy,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred),\n",
        "        'Recall': recall_score(y_test, y_pred),\n",
        "        'F1': f1_score(y_test, y_pred),\n",
        "        'ROC_AUC': roc_auc_score(y_test, y_pred_proba),\n",
        "        'PR_AUC': average_precision_score(y_test, y_pred_proba)\n",
        "    }\n",
        "    \n",
        "    return metrics, y_pred, y_pred_proba\n",
        "\n",
        "# Train and evaluate all model/sampling combinations\n",
        "print(\"\\n🏋️ Training models with different sampling strategies...\")\n",
        "\n",
        "results = []\n",
        "best_models = {}\n",
        "\n",
        "for sampling_name, (X_samp, y_samp) in sampling_strategies.items():\n",
        "    print(f\"\\n📊 Training with {sampling_name} sampling...\")\n",
        "    \n",
        "    # Scale features for algorithms that need it\n",
        "    scaler = StandardScaler()\n",
        "    X_samp_scaled = scaler.fit_transform(X_samp)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    for model_name, model in models.items():\n",
        "        print(f\"  🤖 Training {model_name}...\")\n",
        "        \n",
        "        # Use scaled data for Logistic Regression, original for tree-based models\n",
        "        if 'Logistic' in model_name:\n",
        "            X_train_use, X_test_use = X_samp_scaled, X_test_scaled\n",
        "        else:\n",
        "            X_train_use, X_test_use = X_samp, X_test\n",
        "        \n",
        "        # Train model\n",
        "        model.fit(X_train_use, y_samp)\n",
        "        \n",
        "        # Evaluate\n",
        "        metrics, y_pred, y_pred_proba = evaluate_model_performance(\n",
        "            model, X_test_use, y_test, model_name, sampling_name\n",
        "        )\n",
        "        results.append(metrics)\n",
        "        \n",
        "        # Store best models (based on F1 score for imbalanced data)\n",
        "        key = f\"{model_name}_{sampling_name}\"\n",
        "        best_models[key] = {\n",
        "            'model': model,\n",
        "            'scaler': scaler if 'Logistic' in model_name else None,\n",
        "            'f1_score': metrics['F1'],\n",
        "            'predictions': (y_pred, y_pred_proba)\n",
        "        }\n",
        "        \n",
        "        print(f\"    ✅ F1: {metrics['F1']:.3f}, ROC-AUC: {metrics['ROC_AUC']:.3f}\")\n",
        "\n",
        "print(f\"\\n✅ Training complete! {len(results)} model combinations evaluated\")\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n📊 Results Summary:\")\n",
        "print(results_df.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Model Comparison and Best Model Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best performing models\n",
        "print(\"🏆 Model Performance Analysis...\")\n",
        "\n",
        "# Sort by F1 score (most important for imbalanced classification)\n",
        "best_f1 = results_df.loc[results_df['F1'].idxmax()]\n",
        "best_roc_auc = results_df.loc[results_df['ROC_AUC'].idxmax()]\n",
        "best_precision = results_df.loc[results_df['Precision'].idxmax()]\n",
        "best_recall = results_df.loc[results_df['Recall'].idxmax()]\n",
        "\n",
        "print(f\"\\n🏅 Best F1 Score: {best_f1['Model']} with {best_f1['Sampling']} (F1: {best_f1['F1']:.3f})\")\n",
        "print(f\"🏅 Best ROC-AUC: {best_roc_auc['Model']} with {best_roc_auc['Sampling']} (AUC: {best_roc_auc['ROC_AUC']:.3f})\")\n",
        "print(f\"🏅 Best Precision: {best_precision['Model']} with {best_precision['Sampling']} (Precision: {best_precision['Precision']:.3f})\")\n",
        "print(f\"🏅 Best Recall: {best_recall['Model']} with {best_recall['Sampling']} (Recall: {best_recall['Recall']:.3f})\")\n",
        "\n",
        "# Select best overall model (based on F1 score)\n",
        "best_model_key = f\"{best_f1['Model']}_{best_f1['Sampling']}\"\n",
        "best_model_info = best_models[best_model_key]\n",
        "\n",
        "print(f\"\\n🎯 Selected Best Model: {best_model_key}\")\n",
        "print(f\"   F1 Score: {best_model_info['f1_score']:.3f}\")\n",
        "\n",
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. F1 Score comparison\n",
        "pivot_f1 = results_df.pivot(index='Model', columns='Sampling', values='F1')\n",
        "sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[0,0])\n",
        "axes[0,0].set_title('F1 Score by Model and Sampling Strategy')\n",
        "\n",
        "# 2. ROC-AUC comparison\n",
        "pivot_roc = results_df.pivot(index='Model', columns='Sampling', values='ROC_AUC')\n",
        "sns.heatmap(pivot_roc, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[0,1])\n",
        "axes[0,1].set_title('ROC-AUC by Model and Sampling Strategy')\n",
        "\n",
        "# 3. Precision vs Recall\n",
        "for sampling in results_df['Sampling'].unique():\n",
        "    subset = results_df[results_df['Sampling'] == sampling]\n",
        "    axes[1,0].scatter(subset['Recall'], subset['Precision'], \n",
        "                     label=sampling, alpha=0.7, s=60)\n",
        "axes[1,0].set_xlabel('Recall')\n",
        "axes[1,0].set_ylabel('Precision')\n",
        "axes[1,0].set_title('Precision vs Recall by Sampling Strategy')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Model performance radar chart (for best model)\n",
        "metrics_for_radar = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC_AUC', 'PR_AUC']\n",
        "best_metrics = [best_f1[metric] for metric in metrics_for_radar]\n",
        "\n",
        "# Simple bar chart instead of radar\n",
        "axes[1,1].bar(range(len(metrics_for_radar)), best_metrics, \n",
        "              color=['skyblue', 'lightgreen', 'orange', 'red', 'purple', 'brown'])\n",
        "axes[1,1].set_xticks(range(len(metrics_for_radar)))\n",
        "axes[1,1].set_xticklabels(metrics_for_radar, rotation=45)\n",
        "axes[1,1].set_title(f'Best Model Performance: {best_model_key}')\n",
        "axes[1,1].set_ylim(0, 1)\n",
        "\n",
        "for i, v in enumerate(best_metrics):\n",
        "    axes[1,1].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed performance table\n",
        "print(f\"\\n📊 Detailed Performance Comparison:\")\n",
        "display_df = results_df.copy()\n",
        "for col in ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC_AUC', 'PR_AUC']:\n",
        "    display_df[col] = display_df[col].round(3)\n",
        "    \n",
        "print(display_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Best Model Analysis and Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed analysis of best model\n",
        "print(f\"🔍 Detailed Analysis of Best Model: {best_model_key}\")\n",
        "\n",
        "best_model = best_model_info['model']\n",
        "y_pred_best, y_pred_proba_best = best_model_info['predictions']\n",
        "\n",
        "# 1. Confusion Matrix and Classification Report\n",
        "print(f\"\\n📊 Confusion Matrix and Classification Report:\")\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_best))\n",
        "\n",
        "# 2. Feature Importance (for tree-based models)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    print(f\"\\n🎯 Feature Importance Analysis:\")\n",
        "    \n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': final_feature_names,\n",
        "        'Importance': best_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    print(\"Top 10 Most Important Features:\")\n",
        "    print(feature_importance.head(10).to_string(index=False))\n",
        "    \n",
        "    # Visualize feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = feature_importance.head(15)\n",
        "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
        "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title(f'Top 15 Feature Importances - {best_model_key}')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Compare with EDA insights\n",
        "    print(f\"\\n💡 EDA vs Model Insights:\")\n",
        "    eda_top_features = ['total_principal_ratio', 'total_accounts_in_collections', 'principal_at_placement']\n",
        "    model_top_features = feature_importance.head(5)['Feature'].tolist()\n",
        "    \n",
        "    print(f\"EDA Top Features: {eda_top_features}\")\n",
        "    print(f\"Model Top Features: {model_top_features}\")\n",
        "    \n",
        "    overlap = set(eda_top_features) & set(model_top_features)\n",
        "    print(f\"Overlap: {list(overlap)} ({len(overlap)}/{len(eda_top_features)} features match)\")\n",
        "\n",
        "# 3. ROC Curve and Precision-Recall Curve\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_best)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
        "axes[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[0].set_xlim([0.0, 1.0])\n",
        "axes[0].set_ylim([0.0, 1.05])\n",
        "axes[0].set_xlabel('False Positive Rate')\n",
        "axes[0].set_ylabel('True Positive Rate')\n",
        "axes[0].set_title('ROC Curve - Best Model')\n",
        "axes[0].legend(loc=\"lower right\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba_best)\n",
        "pr_auc = average_precision_score(y_test, y_pred_proba_best)\n",
        "axes[1].plot(recall_vals, precision_vals, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
        "axes[1].axhline(y=y_test.mean(), color='red', linestyle='--', label=f'Baseline ({y_test.mean():.3f})')\n",
        "axes[1].set_xlim([0.0, 1.0])\n",
        "axes[1].set_ylim([0.0, 1.05])\n",
        "axes[1].set_xlabel('Recall')\n",
        "axes[1].set_ylabel('Precision')\n",
        "axes[1].set_title('Precision-Recall Curve - Best Model')\n",
        "axes[1].legend(loc=\"lower left\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion Matrix Heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[2])\n",
        "axes[2].set_title('Confusion Matrix - Best Model')\n",
        "axes[2].set_xlabel('Predicted')\n",
        "axes[2].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Business Impact Analysis\n",
        "print(f\"\\n💼 Business Impact Analysis:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Calculate business metrics\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "total_loans = len(y_test)\n",
        "actual_enrollments = y_test.sum()\n",
        "\n",
        "print(f\"📊 Test Set Analysis:\")\n",
        "print(f\"  Total loans: {total_loans:,}\")\n",
        "print(f\"  Actual enrollments: {actual_enrollments:,} ({actual_enrollments/total_loans:.1%})\")\n",
        "print(f\"  Predicted enrollments: {y_pred_best.sum():,} ({y_pred_best.sum()/total_loans:.1%})\")\n",
        "\n",
        "print(f\"\\n🎯 Model Performance:\")\n",
        "print(f\"  True Positives (Correctly identified enrollments): {tp:,}\")\n",
        "print(f\"  False Positives (Incorrectly predicted enrollments): {fp:,}\")\n",
        "print(f\"  True Negatives (Correctly identified non-enrollments): {tn:,}\")\n",
        "print(f\"  False Negatives (Missed enrollments): {fn:,}\")\n",
        "\n",
        "# Cost-benefit analysis (example)\n",
        "print(f\"\\n💰 Example Cost-Benefit Analysis:\")\n",
        "cost_per_outreach = 50  # Example: $50 per outreach campaign\n",
        "value_per_enrollment = 500  # Example: $500 value per successful enrollment\n",
        "\n",
        "outreach_cost = y_pred_best.sum() * cost_per_outreach\n",
        "enrollment_value = tp * value_per_enrollment\n",
        "false_positive_cost = fp * cost_per_outreach\n",
        "\n",
        "net_benefit = enrollment_value - outreach_cost\n",
        "roi = (net_benefit / outreach_cost * 100) if outreach_cost > 0 else 0\n",
        "\n",
        "print(f\"  Outreach cost: ${outreach_cost:,} ({y_pred_best.sum():,} predicted enrollments × ${cost_per_outreach})\")\n",
        "print(f\"  Enrollment value: ${enrollment_value:,} ({tp:,} true positives × ${value_per_enrollment})\")\n",
        "print(f\"  Net benefit: ${net_benefit:,}\")\n",
        "print(f\"  ROI: {roi:.1f}%\")\n",
        "\n",
        "print(f\"\\n🎉 MODELING COMPLETE!\")\n",
        "print(f\"🏆 Best Model: {best_model_key}\")\n",
        "print(f\"📈 Key Metrics: F1={best_f1['F1']:.3f}, ROC-AUC={best_f1['ROC_AUC']:.3f}, Precision={best_f1['Precision']:.3f}, Recall={best_f1['Recall']:.3f}\")\n",
        "print(f\"📝 Ready for business case analysis in notebook 03!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Model Persistence and Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model and preprocessing components\n",
        "import pickle\n",
        "import joblib\n",
        "\n",
        "print(\"💾 Saving Best Model and Components...\")\n",
        "\n",
        "# Create comprehensive model package with preprocessing artifacts\n",
        "model_package = {\n",
        "    'model': best_model,\n",
        "    'scaler': best_model_info['scaler'],  # None for tree-based models\n",
        "    'preprocessing_artifacts': preprocessing_artifacts,  # Complete preprocessing info from build_features\n",
        "    'final_feature_names': final_feature_names,\n",
        "    'target_column': target_column,\n",
        "    'model_type': best_f1['Model'],\n",
        "    'sampling_strategy': best_f1['Sampling'],\n",
        "    'performance_metrics': {\n",
        "        'f1_score': best_f1['F1'],\n",
        "        'roc_auc': best_f1['ROC_AUC'],\n",
        "        'precision': best_f1['Precision'],\n",
        "        'recall': best_f1['Recall'],\n",
        "        'accuracy': best_f1['Accuracy']\n",
        "    },\n",
        "    'preprocessing_pipeline': 'src.features.build_features.preprocess_data',  # Reference to preprocessing function\n",
        "    'model_version': '1.0',\n",
        "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "model_filename = 'best_loan_enrollment_model.pkl'\n",
        "with open(model_filename, 'wb') as f:\n",
        "    pickle.dump(model_package, f)\n",
        "\n",
        "print(f\"✅ Model saved as: {model_filename}\")\n",
        "\n",
        "# Create detailed summary report\n",
        "print(f\"\\n📋 COMPREHENSIVE MODELING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n🎯 PROBLEM STATEMENT:\")\n",
        "print(f\"  Predict loan enrollment within 180 days\")\n",
        "print(f\"  Dataset: {df_raw.shape[0]:,} loans with {df_raw.shape[1]} features\")\n",
        "print(f\"  Target: {target_column} (16.1% positive rate)\")\n",
        "\n",
        "print(f\"\\n🔧 PREPROCESSING APPLIED (via build_features.py):\")\n",
        "print(f\"  ✅ Removed multicollinear features: {len(preprocessing_artifacts['removed_features'])}\")\n",
        "print(f\"  ✅ Handled missing values in {len(preprocessing_artifacts['imputation_info'])} features\")\n",
        "print(f\"  ✅ Capped outliers in {len(preprocessing_artifacts['outlier_info'])} features\")\n",
        "print(f\"  ✅ Encoded {len(preprocessing_artifacts['label_encoders'])} categorical features\")\n",
        "print(f\"  ✅ Applied 4 sampling strategies for class imbalance\")\n",
        "print(f\"  ✅ Final feature set: {len(final_feature_names)} features\")\n",
        "\n",
        "print(f\"\\n🤖 MODELS EVALUATED:\")\n",
        "print(f\"  ✅ {len(models)} algorithms × {len(sampling_strategies)} sampling strategies = {len(results_df)} combinations\")\n",
        "print(f\"  ✅ Models: {', '.join(models.keys())}\")\n",
        "print(f\"  ✅ Sampling: {', '.join(sampling_strategies.keys())}\")\n",
        "\n",
        "print(f\"\\n🏆 BEST MODEL SELECTED:\")\n",
        "print(f\"  Algorithm: {best_f1['Model']}\")\n",
        "print(f\"  Sampling: {best_f1['Sampling']}\")\n",
        "print(f\"  F1 Score: {best_f1['F1']:.3f}\")\n",
        "print(f\"  ROC-AUC: {best_f1['ROC_AUC']:.3f}\")\n",
        "print(f\"  Precision: {best_f1['Precision']:.3f}\")\n",
        "print(f\"  Recall: {best_f1['Recall']:.3f}\")\n",
        "\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    top_3_features = feature_importance.head(3)['Feature'].tolist()\n",
        "    print(f\"  Top 3 Features: {', '.join(top_3_features)}\")\n",
        "\n",
        "print(f\"\\n💼 BUSINESS IMPACT:\")\n",
        "tp, fp, fn, tn = cm.ravel()\n",
        "print(f\"  True Positives: {tp:,} (correctly identified enrollments)\")\n",
        "print(f\"  False Positives: {fp:,} (unnecessary outreach)\")\n",
        "print(f\"  False Negatives: {fn:,} (missed opportunities)\")\n",
        "print(f\"  Model identifies {tp/(tp+fn):.1%} of actual enrollments\")\n",
        "\n",
        "print(f\"\\n📊 EDA VALIDATION:\")\n",
        "eda_insights = [\n",
        "    \"✅ Addressed class imbalance (16.1% enrollment rate)\",\n",
        "    \"✅ Removed highly correlated features\",\n",
        "    \"✅ Handled missing values strategically\", \n",
        "    \"✅ Validated top predictive features from EDA\",\n",
        "    \"✅ Applied robust outlier handling\"\n",
        "]\n",
        "for insight in eda_insights:\n",
        "    print(f\"  {insight}\")\n",
        "\n",
        "print(f\"\\n🎯 RECOMMENDATIONS:\")\n",
        "print(f\"  1. Use {best_f1['Model']} with {best_f1['Sampling']} sampling\")\n",
        "print(f\"  2. Focus on top features: {', '.join(top_3_features) if 'top_3_features' in locals() else 'tree-based importance'}\")\n",
        "print(f\"  3. Monitor model performance on new data\")\n",
        "print(f\"  4. Consider business costs in threshold selection\")\n",
        "print(f\"  5. Proceed to business case analysis (notebook 03)\")\n",
        "\n",
        "print(f\"\\n🎉 MODEL TRAINING AND EVALUATION COMPLETE!\")\n",
        "print(f\"📝 Next: Run notebook 03 for business case analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bounce",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
